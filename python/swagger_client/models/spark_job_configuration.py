# coding: utf-8

"""
    Hopsworks api

    No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)  # noqa: E501

    OpenAPI spec version: 1.1.0-SNAPSHOT
    
    Generated by: https://github.com/swagger-api/swagger-codegen.git
"""

import pprint
import re  # noqa: F401

import six
from swagger_client.models.local_resource_dto import LocalResourceDTO  # noqa: F401,E501
from swagger_client.models.yarn_job_configuration import YarnJobConfiguration  # noqa: F401,E501


class SparkJobConfiguration(YarnJobConfiguration):
    """NOTE: This class is auto generated by the swagger code generator program.

    Do not edit the class manually.
    """
    """
    Attributes:
      swagger_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    swagger_types = {
        'app_path': 'str',
        'main_class': 'str',
        'args': 'str',
        'properties': 'str',
        'experiment_type': 'str',
        'distribution_strategy': 'str',
        'executor_instances': 'int',
        'executor_cores': 'int',
        'executor_memory': 'int',
        'executor_gpus': 'int',
        'num_ps': 'int',
        'dynamic_allocation_enabled': 'bool',
        'dynamic_allocation_min_executors': 'int',
        'dynamic_allocation_max_executors': 'int',
        'dynamic_allocation_initial_executors': 'int',
        'blacklisting_enabled': 'bool',
        'py_files': 'str',
        'files': 'str',
        'jars': 'str',
        'archives': 'str'
    }

    attribute_map = {
        'app_path': 'appPath',
        'main_class': 'mainClass',
        'args': 'args',
        'properties': 'properties',
        'experiment_type': 'experimentType',
        'distribution_strategy': 'distributionStrategy',
        'executor_instances': 'executorInstances',
        'executor_cores': 'executorCores',
        'executor_memory': 'executorMemory',
        'executor_gpus': 'executorGpus',
        'num_ps': 'numPs',
        'dynamic_allocation_enabled': 'dynamicAllocationEnabled',
        'dynamic_allocation_min_executors': 'dynamicAllocationMinExecutors',
        'dynamic_allocation_max_executors': 'dynamicAllocationMaxExecutors',
        'dynamic_allocation_initial_executors': 'dynamicAllocationInitialExecutors',
        'blacklisting_enabled': 'blacklistingEnabled',
        'py_files': 'pyFiles',
        'files': 'files',
        'jars': 'jars',
        'archives': 'archives'
    }

    def __init__(self, app_path=None, main_class=None, args=None, properties=None, experiment_type=None, distribution_strategy=None, executor_instances=None, executor_cores=None, executor_memory=None, executor_gpus=None, num_ps=None, dynamic_allocation_enabled=None, dynamic_allocation_min_executors=None, dynamic_allocation_max_executors=None, dynamic_allocation_initial_executors=None, blacklisting_enabled=None, py_files=None, files=None, jars=None, archives=None):  # noqa: E501
        """SparkJobConfiguration - a model defined in Swagger"""  # noqa: E501
        self._app_path = None
        self._main_class = None
        self._args = None
        self._properties = None
        self._experiment_type = None
        self._distribution_strategy = None
        self._executor_instances = None
        self._executor_cores = None
        self._executor_memory = None
        self._executor_gpus = None
        self._num_ps = None
        self._dynamic_allocation_enabled = None
        self._dynamic_allocation_min_executors = None
        self._dynamic_allocation_max_executors = None
        self._dynamic_allocation_initial_executors = None
        self._blacklisting_enabled = None
        self._py_files = None
        self._files = None
        self._jars = None
        self._archives = None
        self.discriminator = None
        if app_path is not None:
            self.app_path = app_path
        if main_class is not None:
            self.main_class = main_class
        if args is not None:
            self.args = args
        if properties is not None:
            self.properties = properties
        if experiment_type is not None:
            self.experiment_type = experiment_type
        if distribution_strategy is not None:
            self.distribution_strategy = distribution_strategy
        if executor_instances is not None:
            self.executor_instances = executor_instances
        if executor_cores is not None:
            self.executor_cores = executor_cores
        if executor_memory is not None:
            self.executor_memory = executor_memory
        if executor_gpus is not None:
            self.executor_gpus = executor_gpus
        if num_ps is not None:
            self.num_ps = num_ps
        if dynamic_allocation_enabled is not None:
            self.dynamic_allocation_enabled = dynamic_allocation_enabled
        if dynamic_allocation_min_executors is not None:
            self.dynamic_allocation_min_executors = dynamic_allocation_min_executors
        if dynamic_allocation_max_executors is not None:
            self.dynamic_allocation_max_executors = dynamic_allocation_max_executors
        if dynamic_allocation_initial_executors is not None:
            self.dynamic_allocation_initial_executors = dynamic_allocation_initial_executors
        if blacklisting_enabled is not None:
            self.blacklisting_enabled = blacklisting_enabled
        if py_files is not None:
            self.py_files = py_files
        if files is not None:
            self.files = files
        if jars is not None:
            self.jars = jars
        if archives is not None:
            self.archives = archives

    @property
    def app_path(self):
        """Gets the app_path of this SparkJobConfiguration.  # noqa: E501


        :return: The app_path of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._app_path

    @app_path.setter
    def app_path(self, app_path):
        """Sets the app_path of this SparkJobConfiguration.


        :param app_path: The app_path of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._app_path = app_path

    @property
    def main_class(self):
        """Gets the main_class of this SparkJobConfiguration.  # noqa: E501


        :return: The main_class of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._main_class

    @main_class.setter
    def main_class(self, main_class):
        """Sets the main_class of this SparkJobConfiguration.


        :param main_class: The main_class of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._main_class = main_class

    @property
    def args(self):
        """Gets the args of this SparkJobConfiguration.  # noqa: E501


        :return: The args of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._args

    @args.setter
    def args(self, args):
        """Sets the args of this SparkJobConfiguration.


        :param args: The args of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._args = args

    @property
    def properties(self):
        """Gets the properties of this SparkJobConfiguration.  # noqa: E501


        :return: The properties of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._properties

    @properties.setter
    def properties(self, properties):
        """Sets the properties of this SparkJobConfiguration.


        :param properties: The properties of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._properties = properties

    @property
    def experiment_type(self):
        """Gets the experiment_type of this SparkJobConfiguration.  # noqa: E501


        :return: The experiment_type of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._experiment_type

    @experiment_type.setter
    def experiment_type(self, experiment_type):
        """Sets the experiment_type of this SparkJobConfiguration.


        :param experiment_type: The experiment_type of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """
        allowed_values = ["EXPERIMENT", "PARALLEL_EXPERIMENTS", "DISTRIBUTED_TRAINING"]  # noqa: E501
        if experiment_type not in allowed_values:
            raise ValueError(
                "Invalid value for `experiment_type` ({0}), must be one of {1}"  # noqa: E501
                .format(experiment_type, allowed_values)
            )

        self._experiment_type = experiment_type

    @property
    def distribution_strategy(self):
        """Gets the distribution_strategy of this SparkJobConfiguration.  # noqa: E501


        :return: The distribution_strategy of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._distribution_strategy

    @distribution_strategy.setter
    def distribution_strategy(self, distribution_strategy):
        """Sets the distribution_strategy of this SparkJobConfiguration.


        :param distribution_strategy: The distribution_strategy of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """
        allowed_values = ["MIRRORED", "PARAMETER_SERVER", "COLLECTIVE_ALL_REDUCE"]  # noqa: E501
        if distribution_strategy not in allowed_values:
            raise ValueError(
                "Invalid value for `distribution_strategy` ({0}), must be one of {1}"  # noqa: E501
                .format(distribution_strategy, allowed_values)
            )

        self._distribution_strategy = distribution_strategy

    @property
    def executor_instances(self):
        """Gets the executor_instances of this SparkJobConfiguration.  # noqa: E501


        :return: The executor_instances of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._executor_instances

    @executor_instances.setter
    def executor_instances(self, executor_instances):
        """Sets the executor_instances of this SparkJobConfiguration.


        :param executor_instances: The executor_instances of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._executor_instances = executor_instances

    @property
    def executor_cores(self):
        """Gets the executor_cores of this SparkJobConfiguration.  # noqa: E501


        :return: The executor_cores of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._executor_cores

    @executor_cores.setter
    def executor_cores(self, executor_cores):
        """Sets the executor_cores of this SparkJobConfiguration.


        :param executor_cores: The executor_cores of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._executor_cores = executor_cores

    @property
    def executor_memory(self):
        """Gets the executor_memory of this SparkJobConfiguration.  # noqa: E501


        :return: The executor_memory of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._executor_memory

    @executor_memory.setter
    def executor_memory(self, executor_memory):
        """Sets the executor_memory of this SparkJobConfiguration.


        :param executor_memory: The executor_memory of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._executor_memory = executor_memory

    @property
    def executor_gpus(self):
        """Gets the executor_gpus of this SparkJobConfiguration.  # noqa: E501


        :return: The executor_gpus of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._executor_gpus

    @executor_gpus.setter
    def executor_gpus(self, executor_gpus):
        """Sets the executor_gpus of this SparkJobConfiguration.


        :param executor_gpus: The executor_gpus of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._executor_gpus = executor_gpus

    @property
    def num_ps(self):
        """Gets the num_ps of this SparkJobConfiguration.  # noqa: E501


        :return: The num_ps of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._num_ps

    @num_ps.setter
    def num_ps(self, num_ps):
        """Sets the num_ps of this SparkJobConfiguration.


        :param num_ps: The num_ps of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._num_ps = num_ps

    @property
    def dynamic_allocation_enabled(self):
        """Gets the dynamic_allocation_enabled of this SparkJobConfiguration.  # noqa: E501


        :return: The dynamic_allocation_enabled of this SparkJobConfiguration.  # noqa: E501
        :rtype: bool
        """
        return self._dynamic_allocation_enabled

    @dynamic_allocation_enabled.setter
    def dynamic_allocation_enabled(self, dynamic_allocation_enabled):
        """Sets the dynamic_allocation_enabled of this SparkJobConfiguration.


        :param dynamic_allocation_enabled: The dynamic_allocation_enabled of this SparkJobConfiguration.  # noqa: E501
        :type: bool
        """

        self._dynamic_allocation_enabled = dynamic_allocation_enabled

    @property
    def dynamic_allocation_min_executors(self):
        """Gets the dynamic_allocation_min_executors of this SparkJobConfiguration.  # noqa: E501


        :return: The dynamic_allocation_min_executors of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._dynamic_allocation_min_executors

    @dynamic_allocation_min_executors.setter
    def dynamic_allocation_min_executors(self, dynamic_allocation_min_executors):
        """Sets the dynamic_allocation_min_executors of this SparkJobConfiguration.


        :param dynamic_allocation_min_executors: The dynamic_allocation_min_executors of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._dynamic_allocation_min_executors = dynamic_allocation_min_executors

    @property
    def dynamic_allocation_max_executors(self):
        """Gets the dynamic_allocation_max_executors of this SparkJobConfiguration.  # noqa: E501


        :return: The dynamic_allocation_max_executors of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._dynamic_allocation_max_executors

    @dynamic_allocation_max_executors.setter
    def dynamic_allocation_max_executors(self, dynamic_allocation_max_executors):
        """Sets the dynamic_allocation_max_executors of this SparkJobConfiguration.


        :param dynamic_allocation_max_executors: The dynamic_allocation_max_executors of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._dynamic_allocation_max_executors = dynamic_allocation_max_executors

    @property
    def dynamic_allocation_initial_executors(self):
        """Gets the dynamic_allocation_initial_executors of this SparkJobConfiguration.  # noqa: E501


        :return: The dynamic_allocation_initial_executors of this SparkJobConfiguration.  # noqa: E501
        :rtype: int
        """
        return self._dynamic_allocation_initial_executors

    @dynamic_allocation_initial_executors.setter
    def dynamic_allocation_initial_executors(self, dynamic_allocation_initial_executors):
        """Sets the dynamic_allocation_initial_executors of this SparkJobConfiguration.


        :param dynamic_allocation_initial_executors: The dynamic_allocation_initial_executors of this SparkJobConfiguration.  # noqa: E501
        :type: int
        """

        self._dynamic_allocation_initial_executors = dynamic_allocation_initial_executors

    @property
    def blacklisting_enabled(self):
        """Gets the blacklisting_enabled of this SparkJobConfiguration.  # noqa: E501


        :return: The blacklisting_enabled of this SparkJobConfiguration.  # noqa: E501
        :rtype: bool
        """
        return self._blacklisting_enabled

    @blacklisting_enabled.setter
    def blacklisting_enabled(self, blacklisting_enabled):
        """Sets the blacklisting_enabled of this SparkJobConfiguration.


        :param blacklisting_enabled: The blacklisting_enabled of this SparkJobConfiguration.  # noqa: E501
        :type: bool
        """

        self._blacklisting_enabled = blacklisting_enabled

    @property
    def py_files(self):
        """Gets the py_files of this SparkJobConfiguration.  # noqa: E501


        :return: The py_files of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._py_files

    @py_files.setter
    def py_files(self, py_files):
        """Sets the py_files of this SparkJobConfiguration.


        :param py_files: The py_files of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._py_files = py_files

    @property
    def files(self):
        """Gets the files of this SparkJobConfiguration.  # noqa: E501


        :return: The files of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._files

    @files.setter
    def files(self, files):
        """Sets the files of this SparkJobConfiguration.


        :param files: The files of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._files = files

    @property
    def jars(self):
        """Gets the jars of this SparkJobConfiguration.  # noqa: E501


        :return: The jars of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._jars

    @jars.setter
    def jars(self, jars):
        """Sets the jars of this SparkJobConfiguration.


        :param jars: The jars of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._jars = jars

    @property
    def archives(self):
        """Gets the archives of this SparkJobConfiguration.  # noqa: E501


        :return: The archives of this SparkJobConfiguration.  # noqa: E501
        :rtype: str
        """
        return self._archives

    @archives.setter
    def archives(self, archives):
        """Sets the archives of this SparkJobConfiguration.


        :param archives: The archives of this SparkJobConfiguration.  # noqa: E501
        :type: str
        """

        self._archives = archives

    def to_dict(self):
        """Returns the model properties as a dict"""
        result = {}

        for attr, _ in six.iteritems(self.swagger_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value
        if issubclass(SparkJobConfiguration, dict):
            for key, value in self.items():
                result[key] = value

        return result

    def to_str(self):
        """Returns the string representation of the model"""
        return pprint.pformat(self.to_dict())

    def __repr__(self):
        """For `print` and `pprint`"""
        return self.to_str()

    def __eq__(self, other):
        """Returns true if both objects are equal"""
        if not isinstance(other, SparkJobConfiguration):
            return False

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """Returns true if both objects are not equal"""
        return not self == other
